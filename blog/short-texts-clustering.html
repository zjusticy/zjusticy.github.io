<!DOCTYPE html><html><head><link rel="icon" type="image/png" href="/favicon.png"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-164183122-1"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-164183122-1', {
              page_path: window.location.pathname,
            });
          </script><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><title>A novel method for short text clustering by using word2vec and cosine similarity</title><meta name="description" content="An efficient process for unsupervised clustering the short texts"/><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/60315ad9a90efc0d33fb.css" as="style"/><link rel="stylesheet" href="/_next/static/css/60315ad9a90efc0d33fb.css" data-n-g=""/><link rel="preload" href="/_next/static/css/fc1e4e67462c30525b35.css" as="style"/><link rel="stylesheet" href="/_next/static/css/fc1e4e67462c30525b35.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-a40ef1678bae11e696dba45124eadd70.js"></script><script src="/_next/static/chunks/webpack-90a60b87fd0d5fc150f2.js" defer=""></script><script src="/_next/static/chunks/framework-2191d16384373197bc0a.js" defer=""></script><script src="/_next/static/chunks/main-77a3ef3c4209de166c9f.js" defer=""></script><script src="/_next/static/chunks/pages/_app-2ade07109f2699b8d311.js" defer=""></script><script src="/_next/static/chunks/cb1608f2-42d0501067846f350436.js" defer=""></script><script src="/_next/static/chunks/921-45b9e03a57b55d5da14f.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-87d76c774d32b5089840.js" defer=""></script><script src="/_next/static/Wl-PMQMH630Xr_XU9AKrd/_buildManifest.js" defer=""></script><script src="/_next/static/Wl-PMQMH630Xr_XU9AKrd/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div><nav class="layout_navbar__1zC8E" role="navigation" aria-label="main navigation"><div class="container"><div class="layout_navbarBrand__37nmK"><a class="layout_navbarItem__1LSrv" href="/"><svg width="36" height="36" viewBox="0 0 144 142" aria-hidden="true" fill="#000" class="svg-icon " xmlns="http://www.w3.org/2000/svg"><rect x="124" y="81" width="20" height="20" fill="black"></rect><rect x="124" y="102" width="20" height="20" fill="black"></rect><rect x="41" y="20" width="20" height="20" fill="black"></rect><rect x="83" y="20" width="20" height="20" fill="black"></rect><rect x="62" y="122" width="20" height="20" fill="black"></rect><rect x="124" y="60" width="20" height="20" fill="black"></rect><rect y="81" width="20" height="20" fill="black"></rect><rect y="102" width="20" height="20" fill="black"></rect><rect x="62" width="20" height="20" fill="black"></rect><rect x="41" y="122" width="20" height="20" fill="black"></rect><rect x="104" y="122" width="20" height="20" fill="black"></rect><rect x="104" y="40" width="20" height="20" fill="black"></rect><rect x="83" y="122" width="20" height="20" fill="black"></rect><rect x="20" y="40" width="20" height="20" fill="black"></rect><rect x="20" y="122" width="20" height="20" fill="black"></rect><rect y="60" width="20" height="20" fill="black"></rect></svg></a><a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navbarBasicExample"><span aria-hidden="true"></span><span aria-hidden="true"></span><span aria-hidden="true"></span></a></div><div id="navbarBasicExample" class="layout_navbarMenu__1NXMa"><div class="layout_navEndSettings__227S4 layout_navbarEnd__3jMCV"><a class="layout_navbarItem__1LSrv layout_moreMargin__2KGfE layout_nonBorder__OGLUi" aria-label="Home page" href="/">Home</a><a class="layout_navbarItem__1LSrv layout_moreMargin__2KGfE layout_addBorder__2Y_EI" aria-label="Blog page" href="/blog">Blog</a><a class="layout_navbarItem__1LSrv layout_moreMargin__2KGfE layout_nonBorder__OGLUi" aria-label="About the author" href="/about">About</a></div></div></div></nav><main><div class="container"><div class="isBlog"><nav class="breadcrumb is-medium" aria-label="breadcrumbs"><ul><li><a href="/blog">Blog</a></li><li class="is-active"><span></span></li></ul></nav><article><h1 class="isBlog-title is-2p5">A novel method for short text clustering by using word2vec and cosine similarity</h1><span class="dateStyle-2">26 Dec, 2019</span><div class="content"><div><p><a href="https://github.com/Tong-Zhu/shortTextsClustering">github link</a></p>
<h2>Short text clustering</h2>
<p>Text clustering is to transform the individual document from the original natural language text into mathematical information, which is presented in the form of high-dimensional space points.</p>
<p>Clustering of long texts is easier because they contain a larger amount of words and more features per text, which helps clustering.</p>
<p>However, for short texts, especially in the form of tweets, each sample has fewer features. If the idea in the vector space model is used, the feature vector constructed by each sample will be very sparse. Eventually, the clustering of short texts simply becomes a short text aggregation at the level of "word repetition".</p>
<h2>Word2vec and cosine similarity</h2>
<p>Natural language is a complex system used to express meaning. In this system, words are the basic unit of meaning. As the name implies, a word vector is a vector used to represent a word, and can also be considered as a feature vector or representation of a word. The technique of mapping words to vectors is also called word embedding.</p>
<p>We can use one-hot vector to represent words. Suppose the number of different words in the dictionary (the dictionary size) is N, and  the index of a word is i. To get the one-hot vector representation, we create a vector of length N with all 0s and set its ith bit to 1.</p>
<p>Although the one-hot word vector is easy to construct, it is usually not a good choice. One of the main reasons is that the one-hot word vector does not accurately represent the similarity between different words, such as the cosine similarity we often use.</p>
<blockquote>
<p><strong>Cosine similarity</strong> is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. The cosine of 0° is 1, and it is less than 1 for any angle in the interval (0,π] radians. It is thus a judgment of orientation and not magnitude: two vectors with the same orientation have a cosine similarity of 1, two vectors oriented at 90° relative to each other have a similarity of 0, and two vectors diametrically opposed have a similarity of -1, independent of their magnitude. [wiki]</p>
</blockquote>
<p>The <strong>word2vec</strong> tool is used to solve the above problem. It expresses each word as a fixed-length vector and makes these vectors better express the similarity and analogy between different words.</p>
<h2>Database</h2>
<p>In late 2017, a Clemson University research group began to catalogue Twitter activity from specific accounts. This set included nearly three million Tweets associated with 2848 unique Twitter handles, categorized by the type of content each account focused on. Data was obtained from <a href="https://www.kaggle.com/fivethirtyeight/russian-troll-tweets/">Kaggle</a>.</p>
<p>First, the tweets have been ‘cleaned’ for the convenience of the next steps. We then use <strong>gensim</strong> library in python and this dataset to generate our word2vec model.</p>
<h2>Clustering</h2>
<p>For clustering, we choose the 10,000 most common words from our model and initialize each word with a value of zero. We then project our tweets onto this 10,000x1 vector by using similarity as calculated by our word2vec model [1].</p>
<p><img src="/vectorize_tweet_flow.png" alt="vectorize tweet flow"></p>
<p>For example, for the words ‘OK’ and ’fine’, we choose the 10 most similar words from our model to represent it. Next, if these 10 words can all be found in the set of 10,000 common words, we replace the 0 value of each word with that similarity value; if the value is already non-zero, the larger similarity value is kept. As a result, we are able to transform each tweet into a 10,000x1 sparse vector.</p>
<p>After obtaining these vectors, the cosine similarity was used to compare the closeness between Tweets.</p>
<p><strong>Hierarchical clustering</strong>, also known as <em>hierarchical cluster analysis,</em> is an algorithm that groups similar objects into groups called <em>clusters</em>. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other.</p>
<p>Here, because of the complexity, we’ll use a method similar to hierarchical clustering. For example, we assign a group to the first tweet, then compare the cosine similarity with the rest of the tweets’ vectors. If the cosine similarity is larger than the threshold, we’ll group those Tweets. Then the rest of the tweets’ vectors have to compare the cosine similarity with this group and get an average value to compare to the threshold. However, to simplify the computation, we only compare to the maximum of the first 5 tweets’ vectors in the group for the rest of tweets’ vectors.</p>
<p>We then use <strong>TF-IDF</strong> to extract 10 key words as an topic indicator for the group if the group contains more than 20 tweets’ vectors. From these 10 key words, we can characterize the topic of this group.</p>
<h2>Results</h2>
<p>We clustered a subset of 35,804 from our total tweet set during 6/2017. The threshold for cosine similarity was set to 0.3. From the 10 key words obtained through TF-IDF, we can assess the topic of that group. Here is one group of key words we get:</p>
<p><em>['uk', 'elect', 'may', 'prime', 'minist', 'gambl', 'parliament', 'backfir', 'politician', 'result']</em></p>
<p>Put these words in google, we can easily get the topic we search for:</p>
<p><img src="/topic.png" alt="Topic"></p>
<h2>Further discussion</h2>
<p>We tried <strong>euclidean distance</strong> as well to compare with <strong>cosine similarity</strong>. The first tweet we chose is:</p>
<p><em><strong>“hi rememb prais harvey weinstein wonder human good friend powerhous disavow”</strong></em></p>
<p>Here is some result we get in the order of index:</p>
<table>
<thead>
<tr>
<th>Euclidean distance</th>
<th>Cosine similarity</th>
</tr>
</thead>
<tbody>
<tr>
<td>11 - “hi rememb said weinstein wonder human good friend pow- erhous disavow”</td>
<td>11 - “hi rememb said weinstein wonder human good friend pow- erhous disavow”</td>
</tr>
<tr>
<td>-</td>
<td>223 - “happi thanksgiv good day famili”</td>
</tr>
<tr>
<td>-</td>
<td>357 - “5 day silenc hillari succumb pressur condemn friend wein- stein”</td>
</tr>
<tr>
<td>-</td>
<td>388 - “obama final speak accus rapist friend harvey weinstein”</td>
</tr>
<tr>
<td>515 - “maxin water declar hous human right”</td>
<td>-</td>
</tr>
<tr>
<td>605 - “boom jr ruin woodi allen defend harvey weinstein”</td>
<td>605 - “boom jr ruin woodi allen defend harvey weinstein”</td>
</tr>
<tr>
<td>702 - “ha donna karan regret defend harvey weinstein”</td>
<td>702 - “ha donna karan regret defend harvey weinstein”</td>
</tr>
<tr>
<td>......</td>
<td>......</td>
</tr>
<tr>
<td>-</td>
<td>1377 - lena dunham defend writer accus rape well im realli surpris lena dunham hillari clinton good friend</td>
</tr>
<tr>
<td>......</td>
<td>......</td>
</tr>
</tbody>
</table>
<p>It is obvious that cosine similarity is more accurate and functional.</p>
<p><a href="https://github.com/Tong-Zhu/shortTextsClustering">github link</a></p>
<h2>Reference</h2>
<ol>
<li><a href="https://www.zhihu.com/question/29978268">https://www.zhihu.com/question/29978268</a></li>
</ol></div></div></article><div class="marginTop-2"><a>← Blog</a></div></div></div></main><footer class="layout_myFooter__JubIR"><div class="container"><div>Email me:     <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="envelope" class="svg-inline--fa fa-envelope fa-w-16 " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg> <a href="mailto:tzhu618@gmail.com">tzhu618@gmail.com</a></div><div class="layout_lastColor__ALfcM">© <!-- -->2023<!-- -->, Built with<!-- --> <a href="https://nextjs.org/">NextJs</a> and <a href="https://getbase.org">Base</a></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"title":"A novel method for short text clustering by using word2vec and cosine similarity","date":"2019-10-26T22:40:32.169Z","description":"An efficient process for unsupervised clustering the short texts","slug":"short-texts-clustering","content":"\u003cp\u003e\u003ca href=\"https://github.com/Tong-Zhu/shortTextsClustering\"\u003egithub link\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003eShort text clustering\u003c/h2\u003e\n\u003cp\u003eText clustering is to transform the individual document from the original natural language text into mathematical information, which is presented in the form of high-dimensional space points.\u003c/p\u003e\n\u003cp\u003eClustering of long texts is easier because they contain a larger amount of words and more features per text, which helps clustering.\u003c/p\u003e\n\u003cp\u003eHowever, for short texts, especially in the form of tweets, each sample has fewer features. If the idea in the vector space model is used, the feature vector constructed by each sample will be very sparse. Eventually, the clustering of short texts simply becomes a short text aggregation at the level of \"word repetition\".\u003c/p\u003e\n\u003ch2\u003eWord2vec and cosine similarity\u003c/h2\u003e\n\u003cp\u003eNatural language is a complex system used to express meaning. In this system, words are the basic unit of meaning. As the name implies, a word vector is a vector used to represent a word, and can also be considered as a feature vector or representation of a word. The technique of mapping words to vectors is also called word embedding.\u003c/p\u003e\n\u003cp\u003eWe can use one-hot vector to represent words. Suppose the number of different words in the dictionary (the dictionary size) is N, and  the index of a word is i. To get the one-hot vector representation, we create a vector of length N with all 0s and set its ith bit to 1.\u003c/p\u003e\n\u003cp\u003eAlthough the one-hot word vector is easy to construct, it is usually not a good choice. One of the main reasons is that the one-hot word vector does not accurately represent the similarity between different words, such as the cosine similarity we often use.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eCosine similarity\u003c/strong\u003e is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. The cosine of 0° is 1, and it is less than 1 for any angle in the interval (0,π] radians. It is thus a judgment of orientation and not magnitude: two vectors with the same orientation have a cosine similarity of 1, two vectors oriented at 90° relative to each other have a similarity of 0, and two vectors diametrically opposed have a similarity of -1, independent of their magnitude. [wiki]\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe \u003cstrong\u003eword2vec\u003c/strong\u003e tool is used to solve the above problem. It expresses each word as a fixed-length vector and makes these vectors better express the similarity and analogy between different words.\u003c/p\u003e\n\u003ch2\u003eDatabase\u003c/h2\u003e\n\u003cp\u003eIn late 2017, a Clemson University research group began to catalogue Twitter activity from specific accounts. This set included nearly three million Tweets associated with 2848 unique Twitter handles, categorized by the type of content each account focused on. Data was obtained from \u003ca href=\"https://www.kaggle.com/fivethirtyeight/russian-troll-tweets/\"\u003eKaggle\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFirst, the tweets have been ‘cleaned’ for the convenience of the next steps. We then use \u003cstrong\u003egensim\u003c/strong\u003e library in python and this dataset to generate our word2vec model.\u003c/p\u003e\n\u003ch2\u003eClustering\u003c/h2\u003e\n\u003cp\u003eFor clustering, we choose the 10,000 most common words from our model and initialize each word with a value of zero. We then project our tweets onto this 10,000x1 vector by using similarity as calculated by our word2vec model [1].\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/vectorize_tweet_flow.png\" alt=\"vectorize tweet flow\"\u003e\u003c/p\u003e\n\u003cp\u003eFor example, for the words ‘OK’ and ’fine’, we choose the 10 most similar words from our model to represent it. Next, if these 10 words can all be found in the set of 10,000 common words, we replace the 0 value of each word with that similarity value; if the value is already non-zero, the larger similarity value is kept. As a result, we are able to transform each tweet into a 10,000x1 sparse vector.\u003c/p\u003e\n\u003cp\u003eAfter obtaining these vectors, the cosine similarity was used to compare the closeness between Tweets.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eHierarchical clustering\u003c/strong\u003e, also known as \u003cem\u003ehierarchical cluster analysis,\u003c/em\u003e is an algorithm that groups similar objects into groups called \u003cem\u003eclusters\u003c/em\u003e. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other.\u003c/p\u003e\n\u003cp\u003eHere, because of the complexity, we’ll use a method similar to hierarchical clustering. For example, we assign a group to the first tweet, then compare the cosine similarity with the rest of the tweets’ vectors. If the cosine similarity is larger than the threshold, we’ll group those Tweets. Then the rest of the tweets’ vectors have to compare the cosine similarity with this group and get an average value to compare to the threshold. However, to simplify the computation, we only compare to the maximum of the first 5 tweets’ vectors in the group for the rest of tweets’ vectors.\u003c/p\u003e\n\u003cp\u003eWe then use \u003cstrong\u003eTF-IDF\u003c/strong\u003e to extract 10 key words as an topic indicator for the group if the group contains more than 20 tweets’ vectors. From these 10 key words, we can characterize the topic of this group.\u003c/p\u003e\n\u003ch2\u003eResults\u003c/h2\u003e\n\u003cp\u003eWe clustered a subset of 35,804 from our total tweet set during 6/2017. The threshold for cosine similarity was set to 0.3. From the 10 key words obtained through TF-IDF, we can assess the topic of that group. Here is one group of key words we get:\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e['uk', 'elect', 'may', 'prime', 'minist', 'gambl', 'parliament', 'backfir', 'politician', 'result']\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003ePut these words in google, we can easily get the topic we search for:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/topic.png\" alt=\"Topic\"\u003e\u003c/p\u003e\n\u003ch2\u003eFurther discussion\u003c/h2\u003e\n\u003cp\u003eWe tried \u003cstrong\u003eeuclidean distance\u003c/strong\u003e as well to compare with \u003cstrong\u003ecosine similarity\u003c/strong\u003e. The first tweet we chose is:\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003e“hi rememb prais harvey weinstein wonder human good friend powerhous disavow”\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eHere is some result we get in the order of index:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eEuclidean distance\u003c/th\u003e\n\u003cth\u003eCosine similarity\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e11 - “hi rememb said weinstein wonder human good friend pow- erhous disavow”\u003c/td\u003e\n\u003ctd\u003e11 - “hi rememb said weinstein wonder human good friend pow- erhous disavow”\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-\u003c/td\u003e\n\u003ctd\u003e223 - “happi thanksgiv good day famili”\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-\u003c/td\u003e\n\u003ctd\u003e357 - “5 day silenc hillari succumb pressur condemn friend wein- stein”\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-\u003c/td\u003e\n\u003ctd\u003e388 - “obama final speak accus rapist friend harvey weinstein”\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e515 - “maxin water declar hous human right”\u003c/td\u003e\n\u003ctd\u003e-\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e605 - “boom jr ruin woodi allen defend harvey weinstein”\u003c/td\u003e\n\u003ctd\u003e605 - “boom jr ruin woodi allen defend harvey weinstein”\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e702 - “ha donna karan regret defend harvey weinstein”\u003c/td\u003e\n\u003ctd\u003e702 - “ha donna karan regret defend harvey weinstein”\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e......\u003c/td\u003e\n\u003ctd\u003e......\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-\u003c/td\u003e\n\u003ctd\u003e1377 - lena dunham defend writer accus rape well im realli surpris lena dunham hillari clinton good friend\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e......\u003c/td\u003e\n\u003ctd\u003e......\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eIt is obvious that cosine similarity is more accurate and functional.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/Tong-Zhu/shortTextsClustering\"\u003egithub link\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003eReference\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"https://www.zhihu.com/question/29978268\"\u003ehttps://www.zhihu.com/question/29978268\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"short-texts-clustering"},"buildId":"Wl-PMQMH630Xr_XU9AKrd","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>